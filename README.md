## SPARK with Python


Apache Spark partitions data and computations over a cluster with multiple nodes. This makes it easier to work with very large datasets because each node only works with a small amount of data. As each node operates on a subset of the data, it also performs part of the calculations required so that both data processing and computation are performed in parallel over the nodes in a cluster.

---
This is a collection of PySpark exercises, including: 


* Resilient Distributed Dataset (RDD)


* MapReduce 


* SQL and DataFrames 


* Machine Learning


## Reports
[Notebook with Code](http://nbviewer.jupyter.org/github/humburgc/apache_spark/blob/master/spark_with_python.ipynb)